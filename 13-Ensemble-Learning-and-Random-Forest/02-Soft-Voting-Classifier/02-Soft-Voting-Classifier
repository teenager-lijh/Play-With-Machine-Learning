# Soft-Voting-Classifier

完全少数服从多数最终的结果或许并不会太好，所以每个算法的投票应该基于一定的权重。

==> 带权投票 ==> 要求每一个模型都能估计概率

==> 在 `scikit-learn` 中有 `predict_proba` 函数则能估计概率



## Hard Voting 的投票结果

模型1  和 模型4 投该样本为 A 类

其他模型投该样本为 B 类，并没有使用加权的方式

![image-20220512160300650](02-Soft-Voting-Classifier.assets/image-20220512160300650.png)



## Soft Voting

对于一个样本数数据而言，每个模型预测该样本为 A 或者 B 的概率如下，且对概率进行求均值后的结果如右下所示，则最终输出结果为概率大的一方：

![image-20220512160513150](02-Soft-Voting-Classifier.assets/image-20220512160513150.png)



## 算法是否支持概率的分析

### 逻辑回归

支持计算一个样本所属类别的概率

逻辑回归本身就是基于概率模型的

这是逻辑回归中的 `sigmoid` 函数

![image-20220512161116698](02-Soft-Voting-Classifier.assets/image-20220512161116698.png)



### KNN 算法

支持计算一个样本所属类别的概率

对于需要预测的绿色样本：

KNN 算法也是通过在特征空间中进行投票后给出一个预测结果的，那么 KNN 算法估计一个样本的所属类别的概率的方式就是  `p = 投票所属类别除以总票数`

![image-20220512161239290](02-Soft-Voting-Classifier.assets/image-20220512161239290.png)



## 决策树

支持计算一个样本所属类别的概率

概率计算方式：当来了一个新的样本的时候，该样本会顺着树一路向下，当走到叶子节点的时候，分类决策树输出预测所属类别的方式也是在叶子节点上投票后决定的，所以分类决策树给出一个样本预测为某个类别的概率方式与 KNN 相似  `p = 投票所属类别除以总票数`

在决策树的叶子节点中 ==> 信息熵 或 基尼系数 很有可能是不为 0 的 

![image-20220512161540820](02-Soft-Voting-Classifier.assets/image-20220512161540820.png)



### SVM

SVM 可以通过其他的方式来 间接计算出这个分类的概率 ==> 未讨论

可参考 scikit-learn 的官方文档，对该问题的阐述：

[http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)



## Soft Voting 代码实现

代码实现见同目录下的 Jupyter Notebook 文件





## 当前集成学习的问题

即使集成了很多的机器学习算法，但是机器学习算法是优先的，那么如何才能在有限的机器学习算法集成后得到一个更准确的分类结果呢？

见下一小节 : )