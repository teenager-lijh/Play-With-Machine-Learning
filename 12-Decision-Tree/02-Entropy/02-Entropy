# Entropy

Entropy 信息熵

![image-20220511172321898](02-Entropy.assets/image-20220511172321898.png)

问题：

1. 每个节点在哪个维度做划分？
2. 某个维度在哪个值上做划分？

具体来看：其中 x=2.4 和 y=1.8 这两个值是怎么来的？

解决方案：其中一种方法，使用信息熵来解决这个问题。

信息熵：熵在信息论中代表随机变量的不确定度的度量。信息熵越大，数据的不确定性越高，信息熵越小，数据的不确定性越低。



信息熵的计算公式：
$$
H=-\sum_{i=1}^{k} p_{i} \log \left(p_{i}\right)
$$
解释：在分类任务中，其中 pi 代表第 i 个类别的样本占总样本的比例值。因为 pi 一定是一个小于 1 的值，所以 log(pi) 一定是一个负数，那么最后取反得到一个正值。

举个例子：
$$
\begin{aligned}
\left\{\frac{1}{3}, \frac{1}{3}, \frac{1}{3}\right\} \\
H &=-\frac{1}{3} \log \left(\frac{1}{3}\right)-\frac{1}{3} \log \left(\frac{1}{3}\right)-\frac{1}{3} \log \left(\frac{1}{3}\right) \\
&=1.0986
\end{aligned}
$$

$$
\begin{aligned}
&\left\{\frac{1}{10}, \frac{2}{10}, \frac{7}{10}\right\} \\
&H=-\frac{1}{10} \log \left(\frac{1}{10}\right)-\frac{2}{10} \log \left(\frac{2}{10}\right)-\frac{7}{10} \log \left(\frac{7}{10}\right) \\
&\quad=0.8018
\end{aligned}
$$

现在说明了第 2 个例子比第 1 个例子的数据要更确定，从第二个例子来看，其中有一类的数据占了总体数据的 70%，所以第二个例子的数据确定性更大。



更极端的例子：
$$
\begin{aligned}
&\{1,0,0\} \\
&H=-1 \cdot \log (1)=0
\end{aligned}
$$
所有的数据都是同一个类别的，所以是完全确定的，那么信息熵的值就达到了 0，说明它的混乱程度为 0。



如果数据的类别仅有两类：

这时候类别 1 的比例为 p 那么类别 2 的比例则为 1-p

信息熵公式可写为：
$$
\begin{gathered}
H=-\sum_{i=1}^{k} p_{i} \log \left(p_{i}\right) \\
H=-x \log (x)-(1-x) \log (1-x)
\end{gathered}
$$


## 绘制双变量的 entropy 函数

```python
import numpy as np
import matplotlib.pyplot as plt

# 双变量的 entropy 函数
def entropy(p):
    return -p * np.log(p) - (1-p) * np.log(1-p)
```

```python
# 生成一系列的 x 的值
# 第 1 个变量的值是 p1=x，第 2 个变量的值是 p2=1-x
# pi 不能等于 0 ==> 否则会计算出: log(0)->负无穷
x = np.linspace(0.01, 0.99, 200)

plt.plot(x, entropy(x))
plt.show()
```

![image-20220511182740189](02-Entropy.assets/image-20220511182740189.png)

解释：函数的图像说明了，当某一个类别的比例为 100% 的时候，信息熵的值达到最小，这时候也就是最稳定的状态。若某一类的比例占比越大，那么信息熵越低，因为某一类占比越多说明它在某种程度上是更稳定的，因为基本上随便来一个数据，它大概率就是属于那个占比最大的类别的，如果大家的比例都是一样的，那么这是最难确定新来的数据属于哪一个类别的情况，因为它们都是等概率的，这时候信息熵也就达到了最大值，也就是最不确定的状态。



回到最初的问题：

![image-20220511183335492](02-Entropy.assets/image-20220511183335492.png)

问题：

1. 每个节点在哪个维度做划分？
2. 某个维度在哪个值上做划分？

损失函数 = 信息熵

寻找最好的划分方式也就等价于寻找到一个划分方式可以使得信息熵的值达到最小。

解决思路：搜索所有可能的划分方式，找到最好的那个划分方式，也就是找到可以使得信息熵最小的那个划分方式。