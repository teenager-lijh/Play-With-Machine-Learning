# What-is-Decision-Tree

## 什么是决策树

什么是决策树？如：一个算法工程师的招聘？

对于这个例子而言，每个节点（蓝色节点）都是可以通过 yes 或者 no 来表达的。

其中 depth = 3

![image-20220511155152740](01-What-is-Decision-Tree.assets/image-20220511155152740.png)



## 节点是数值的决策树

对于不能够直接回答 yes 或 no 的数据应该怎么办？也就是蓝色的节点上是一个数值。

创建决策树模型：

```python
import numpy as np
import matplotlib.pyplot as plt
```

```python
# 导入鸢尾花数据集
from sklearn import datasets

iris = datasets.load_iris()
X = iris.data[:, 2:]
y = iris.target 

plt.scatter(X[y==0, 0], X[y==0, 1])
plt.scatter(X[y==1, 0], X[y==1, 1])
plt.scatter(X[y==2, 0], X[y==2, 1])
plt.show()
```

![image-20220511161417279](01-What-is-Decision-Tree.assets/image-20220511161417279.png)

```python
from sklearn.tree import DecisionTreeClassifier

# 创建决策树分类器 ==> entropy=熵 ; max_depth=最大深度
# criterion="entropy" 使用信息熵的方式对数据划分
dt_clf = DecisionTreeClassifier(max_depth=2, criterion="entropy", random_state=42)
dt_clf.fit(X, y)
```

```python
# 绘制模型的边界线
def plot_decision_boundary(model, axis):
    
    x0, x1 = np.meshgrid(
        np.linspace(axis[0], axis[1], int((axis[1]-axis[0])*100)).reshape(-1, 1),
        np.linspace(axis[2], axis[3], int((axis[3]-axis[2])*100)).reshape(-1, 1),
    )
    X_new = np.c_[x0.ravel(), x1.ravel()]

    y_predict = model.predict(X_new)
    zz = y_predict.reshape(x0.shape)

    from matplotlib.colors import ListedColormap
    custom_cmap = ListedColormap(['#EF9A9A','#FFF59D','#90CAF9'])
    
    plt.contourf(x0, x1, zz, cmap=custom_cmap)
```

```python
# 绘制决策树的决策边界
plot_decision_boundary(dt_clf, axis=[0.5, 7.5, 0, 3])
plt.scatter(X[y==0,0], X[y==0,1])
plt.scatter(X[y==1,0], X[y==1,1])
plt.scatter(X[y==2,0], X[y==2,1])
plt.show()
```

![image-20220511161459195](01-What-is-Decision-Tree.assets/image-20220511161459195.png)

分别用 A B C 代表三个类别:

1. A ==> 左侧
2. B ==> 右上
3. C ==> 右下

当决策树面对的属性是数值类型的时候，对于 yes 或 no 的回答方法：

![image-20220511161836485](01-What-is-Decision-Tree.assets/image-20220511161836485.png)

1. 在根节点的时候，选择 x 轴的维度，将整个数据集分成两个类别
2. 在第二个蓝色节点的时候，选择 y 轴的维度，将右边的区域又分成了两个类别

## 决策树的特点

1. 非参数学习算法
2. 可以解决分类问题
3. 天然可以解决多分类问题
4. 也可以解决回归问题 ==> 使用最终落在红色的叶子节点上的所有样本的平均值作为回归预测的输出值
5. 有非常好的可解释性，当一个样本进来后，顺着根节点一路向下，每个分叉都有具体的衡量标准，这就是可解释性

## 如何构建决策树

问题是：如何构建决策树？

1. 每个蓝色节点应该在哪个维度做划分？
2. 某个维度在哪个具体的值上做划分？

