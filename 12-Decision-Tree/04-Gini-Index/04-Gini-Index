# Gini-Index

Gini-Index : 基尼系数
$$
G=1-\sum_{i=1}^{k} p_{i}^{2}
$$
基尼系数的性质：

1. 基尼系数越大 ==> 不确定程度越大，越混乱
2. 基尼系数越小 ==> 不确定程度越小，说明确定程度更高

对于追求稳定性而言，基尼系数越小越好



举个例子：

example_1
$$
\begin{aligned}
&\left\{\frac{1}{3}, \frac{1}{3}, \frac{1}{3}\right\} \\
&G=1-\left(\frac{1}{3}\right)^{2}-\left(\frac{1}{3}\right)^{2}-\left(\frac{1}{3}\right)^{2} \\
&\quad=0.6666
\end{aligned}
$$

example_2
$$
\begin{aligned}
&\left\{\frac{1}{10}, \frac{2}{10}, \frac{7}{10}\right\} \\
&G=1-\left(\frac{1}{10}\right)^{2}-\left(\frac{2}{10}\right)^{2}-\left(\frac{7}{10}\right)^{2} \\
&\quad=0.46
\end{aligned}
$$

example_3
$$
\begin{aligned}
&\{1,0,0\} \\
&G=1-1^{2}=0
\end{aligned}
$$



通过在对信息熵进行理解的时候使用的例子，可以得出，基尼系数与信息熵计算出的结果是相似的，都是概率相等的时候达到不确定性的最大值，而当某一个类别的占比比较大的时候，数据整体更偏向于某一个类别，这时候基尼系数更小，可以得出系统更加稳定。



## 绘制基尼系数的图像

绘制两个维度的基尼系数的图像:
$$
\begin{aligned}
G &=1-\sum_{i=1}^{k} p_{i}^{2} \\
G &=1-x^{2}-(1-x)^{2} \\
&=1-x^{2}-1+2 x-x^{2} \\
&=-2 x^{2}+2 x
\end{aligned}
$$
可以看出这是一个抛物线，当 x=1/2 的时候，达到基尼系数的最大值，也就是说明：各个类别的占比大小相等的时候，该系统的不稳定程度达到最大，也就是最混乱。

当某个类别的样本占比偏大的时候，基尼系数会增大，也就说明了 ==> 该样本数据集向某一类别偏斜。



## 使用基尼系数创建决策树

此过程与使用信息熵进行划分模拟的代码非常类似，那么这个过程的代码可见当前目录下的 Jupyter Notebook 中的代码。



## 信息熵 VS 基尼系数

1. 信息熵的计算比基尼系数稍慢 ==> 因为信息熵中存在非线性函数 log 的计算，所以会慢一点
2. scikit-learn 中默认使用基尼系数作为划分衡量的标准
3. 大多数情况下二者没有特别大的效果优劣
